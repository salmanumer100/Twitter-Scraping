#By using the “snscrape” Library, Scrape the twitter data 


pip install snscrape
import snscrape.modules.twitter as sntwitter
import pandas as pd

url = "https://mobile.twitter.com/search?q=from%3Asnscrape%20since%3A2020-01-01%20until%3A2020-12-31&src=typed_query"

def scrape_twitter(url):
    tweets_list = []
    for tweet in sntwitter.TwitterSearchScraper(url).get_items():
        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])
    tweets_df = pd.DataFrame(tweets_list, columns=["Datetime", "Tweet Id", "Text", "Username"])
    return tweets_df

tweets_df = scrape_twitter(url)
print(tweets_df)

#Create a dataframe with date, id, url, tweet content, user,reply count, retweet count,language, source

def scrape_twitter(url):
    tweets_list = []
    for tweet in sntwitter.TwitterSearchScraper(url).get_items():
        tweets_list.append([
            tweet.date.strftime("%Y-%m-%d %H:%M:%S"),
            tweet.id,
            tweet.url,
            tweet.content,
            tweet.user.username,
            tweet.replyCount,
            tweet.retweetCount,
            tweet.lang,
            tweet.sourceLabel
        ])
    tweets_df = pd.DataFrame(
        tweets_list, 
        columns=[
            "Date", 
            "ID", 
            "URL", 
            "Tweet Content", 
            "User", 
            "Reply Count", 
            "Retweet Count", 
            "Language", 
            "Source"
        ]
    )
    return tweets_df

tweets_df = scrape_twitter(url)
print(tweets_df)

#Store each collection of data into a document into Mongodb along with the hashtag or key word we use to  Scrape from twitter. 

pip install pymongo
import pymongo
from pymongo import MongoClient
# To be replaced
client = MongoClient('mongodb://localhost:27017')
#(or)
client = MongoClient("<YOUR_MONGODB_URI>")


db = client["twitter_db"]
collection = db["tweets"]
def scrape_twitter_and_save(url, keyword):
    tweets_list = []
    for tweet in sntwitter.TwitterSearchScraper(url).get_items():
        tweets_list.append([
            tweet.date.strftime("%Y-%m-%d %H:%M:%S"),
            tweet.id,
            tweet.url,
            tweet.content,
            tweet.user.username,
            tweet.replyCount,
            tweet.retweetCount,
            tweet.lang,
            tweet.sourceLabel,
            keyword  # add the keyword to the list of columns
        ])
    tweets_df = pd.DataFrame(
        tweets_list, 
        columns=[
            "Date", 
            "ID", 
            "URL", 
            "Tweet Content", 
            "User", 
            "Reply Count", 
            "Retweet Count", 
            "Language", 
            "Source",
            "Keyword"  # add the keyword to the list of columns
        ]
    )
    # insert the tweets into MongoDB
    collection.insert_many(tweets_df.to_dict("records"))
    return tweets_df

keyword = "snscrape"
tweets_df = scrape_twitter_and_save(url, keyword)
print(tweets_df)

#Create a GUI using streamlit that should contain the feature to enter the keyword or Hashtag to be searched, select the date range and limit
#the tweet count need to be scraped. After scraping, the data needs to be 
#displayed in the page and need a button to upload the data into Database and download the data into csv and json format.


pip install streamlit snscrape pandas pymongo
import streamlit as st
import snscrape.modules.twitter as sntwitter
import pandas as pd
from pymongo import MongoClient
from datetime import datetime, timedelta


# To be replaced
client = MongoClient('mongodb://localhost:27017')
#(or)
client = MongoClient("<YOUR_MONGODB_URI>")

db = client["twitter_db"]
collection = db["tweets"]



def scrape_and_save_tweets(keyword, start_date, end_date, tweet_count):
    url = f"#{keyword} since:{start_date} until:{end_date}"
    tweets_list = []
    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(url).get_items()):
        if i >= tweet_count:
            break
        tweets_list.append([
            tweet.date.strftime("%Y-%m-%d %H:%M:%S"),
            tweet.id,
            tweet.url,
            tweet.content,
            tweet.user.username,
            tweet.replyCount,
            tweet.retweetCount,
            tweet.lang,
            tweet.sourceLabel,
            keyword
        ])
    tweets_df = pd.DataFrame(
        tweets_list, 
        columns=[
            "Date", 
            "ID", 
            "URL", 
            "Tweet Content", 
            "User", 
            "Reply Count", 
            "Retweet Count", 
            "Language", 
            "Source",
            "Keyword"
        ]
    )
    collection.insert_many(tweets_df.to_dict("records"))
    return tweets_df


st.title("Twitter Data Scraper and Uploader")

keyword = st.text_input("Enter the keyword or hashtag to search for:")
start_date = st.date_input("Select the start date:")
end_date = st.date_input("Select the end date:")
tweet_count = st.number_input("Enter the number of tweets to scrape:", min_value=1, max_value=1000, step=1)

if st.button("Scrape Tweets"):
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")
    tweets_df = scrape_and_save_tweets(keyword, start_date_str, end_date_str, tweet_count)
    st.write(tweets_df)

if st.button("Download CSV"):
    tweets = collection.find({"Keyword": keyword})
    tweets_df = pd.DataFrame(tweets)
    st.download_button("Download CSV", tweets_df.to_csv(), file_name=f"{keyword}.csv", mime="text/csv")

if st.button("Download JSON"):
    tweets = collection.find({"Keyword": keyword})
    tweets_df = pd.DataFrame(tweets)
    st.download_button("Download JSON", tweets_df.to_json(), file_name=f"{keyword}.json", mime="application/json")


